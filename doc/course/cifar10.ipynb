{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "# sys.path.append('/home/aistudio/external-libraries') # 添加自定义依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、 加载相关库文件，并打印一下paddle的环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载相关库\n",
    "import os\n",
    "import paddle\n",
    "import paddle.vision.transforms as T\n",
    "from paddle.nn import Layer\n",
    "from paddle.vision.datasets import Cifar10\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "from paddle.static import InputSpec\n",
    "import numpy as np\n",
    "from visualdl import LogWriter # 用来可视化的库\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "# 1 打印环境\n",
    "print(paddle.__version__) # 2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、查看一下数据\n",
    "* 了解训练数据的基本形状和大小\n",
    "* 了解训练集和测试集的大小\n",
    "* 计算训练集数据的均值和方差（不能使用测试集，因为测试环境往往是未知的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 查看一下数据\n",
    "\n",
    "# 观察少量数据\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_dataset = Cifar10(mode='train')\n",
    "plt.figure()\n",
    "idx = 0\n",
    "for img, label in train_dataset: # 产看一个数据五个图像\n",
    "    if idx==0:\n",
    "        print(img)\n",
    "        print(label)\n",
    "    plt.subplot(1,5,idx+1)\n",
    "    plt.imshow(img)\n",
    "    idx += 1\n",
    "    if idx==5:\n",
    "        break\n",
    "plt.show()\n",
    "# img = paddle.transpose(img,perm=[1,2,0]) # [CHW]变换维度成[HWC]，因为这里没用transpose所以不用变换维度\n",
    "\n",
    "# 观察整个数据集及数据分布\n",
    "train_dataset = Cifar10(mode='train',transform=T.ToTensor()) # T.ToTensor是将图片变换通道为[CHW]，并将数据压缩到(0,1)范围\n",
    "test_dataset = Cifar10(mode='test',transform=T.ToTensor())\n",
    "print('一个训练数据:\\n {}'.format(train_dataset[0])) # Tensor(shape=[3, 32, 32], dtype=float32, place=CPUPlace, stop_gradient=True, ...\n",
    "print('训练集大小: {}'.format(len(train_dataset))) # 50k\n",
    "print('一个测试数据:\\n {}'.format(test_dataset[0])) # 一般测试数据是没有标签的\n",
    "print('测试集大小: {}'.format(len(test_dataset))) # 10k\n",
    "\n",
    "means = paddle.zeros([3])\n",
    "stds = paddle.zeros([3])\n",
    "for img, _ in train_dataset:\n",
    "    for d in range(3):\n",
    "        means[d] += img[d,:,:].numpy().mean()\n",
    "        stds[d] += img[d,:,:].numpy().std()\n",
    "means = means.numpy()/len(train_dataset)\n",
    "stds = stds.numpy()/len(train_dataset)\n",
    "print('均值: {}'.format(means)) # [0.491401   0.4821591  0.44653094]\n",
    "print('标准差: {}'.format(stds)) # [0.20220289 0.1993163  0.20086345]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、自定义数据集\n",
    "* 定义继承自paddle.io.Dataset的数据类\n",
    "* 初始化函数__init__里需要用mode区分训练/验证/测试\n",
    "* 初始化函数__init__里需要定义数据增广方法\n",
    "* 编写根据索引idx获取单个数据的逻辑(__getitem__)\n",
    "* 编写获取数据集长度的函数(__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 自定义数据集(自己定义数据集) 与上面的构建dataset等价\n",
    "class MyDateset(paddle.io.Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDateset, self).__init__()\n",
    "\n",
    "        # 3.1 加载原始数据，并定义数据预处理transform\n",
    "        stats = ((0.491401, 0.4821591, 0.44653094), (0.20220289, 0.1993163, 0.20086345)) # 这是上面统计到的训练集的均值和标准差\n",
    "        if mode == 'train':\n",
    "            self.data = Cifar10(mode='train')\n",
    "            self.transform = T.Compose([\n",
    "                T.RandomCrop(32, padding=4), # 随机裁剪\n",
    "                T.RandomHorizontalFlip(), # 水平翻转\n",
    "                T.ToTensor(), # 切换图像通道到(CHW)，并数据压缩到(0,1)范围，还有一些特性请查看文档\n",
    "                T.Normalize(*stats) # 归一化，其他参数与特性查看文档\n",
    "                # 可以加入其他数据预处理方法\n",
    "                # （TensorFlow与Pytorch也有类似的数据预处理函数，每个框架不一样，需要查看文档说明）\n",
    "            ])\n",
    "        elif mode == 'valid' or mode == 'eval': # 验证集和测试集的数据预处理方法不应包括训练集用到的增广的方法\n",
    "            self.data = Cifar10(mode='test')\n",
    "            self.transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(*stats)\n",
    "            ])\n",
    "        else:\n",
    "            raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\n",
    "    \n",
    "    def __getitem__(self, idx): # 按照索引获取一个数据\n",
    "        image = self.data[idx][0]\n",
    "        label = self.data[idx][1]\n",
    "        image = self.transform(image) # 上面是定义数据预处理方法，这里用来处理每个数据\n",
    "        return image,label\n",
    "\n",
    "    def __len__(self): # 返回数据集长度\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、封装成Dataloader并验证\n",
    "* 验证自定义数据集\n",
    "* 查看自定义数据集形状和大小（这里和原始数据查看不重复，每一步都需要验证是否有错误，才能避免多踩坑）\n",
    "* 封装成Dataloader并查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDateset(mode='train')\n",
    "test_dataset = MyDateset(mode='eval')\n",
    "print(train_dataset[0]) # ( Tensor(shape=[3, 32, 32], dtype=float32), array(6, dtype=int64) ) \n",
    "print(len(train_dataset)) # 50k\n",
    "print(test_dataset[0]) # ( Tensor(shape=[3, 32, 32], dtype=float32), array(3, dtype=int64) )\n",
    "print(len(test_dataset)) # 10k\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# 核对一个batch数据\n",
    "for img, label in train_loader(): # 取一个批次的数据\n",
    "    print(img.numpy())\n",
    "    print(label.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、搭建网络模型\n",
    "* 初始化需要的组件(__init__)\n",
    "* 编写数据流向，前向推理(forward)\n",
    "* 核对网络模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 搭建网络模型\n",
    "class MyModel(Layer):\n",
    "    def __init__(self,  num_classes=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=32, kernel_size=3, stride=1) # 30*30\n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2) # 15*15\n",
    "\n",
    "        self.conv2 = Conv2D(in_channels=32, out_channels=64, kernel_size=3, stride=1) # 13*13\n",
    "        self.relu2 = paddle.nn.ReLU()\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2) # 6*6\n",
    "\n",
    "        self.conv3 = Conv2D(in_channels=64, out_channels=64, kernel_size=3, stride=1) # 4*4\n",
    "        self.relu3 = paddle.nn.ReLU()\n",
    "\n",
    "        self.linear1 = Linear(in_features=1024, out_features=64)\n",
    "        self.relu4 = paddle.nn.ReLU()\n",
    "        self.linear2 = Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = paddle.flatten(x, start_axis=1, stop_axis=-1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "inputs = InputSpec([None, 3*32*32], 'float32', 'x')\n",
    "labels = InputSpec([None, 10], 'float32', 'x')\n",
    "model = paddle.Model(MyModel(num_classes=10), inputs, labels)\n",
    "\n",
    "# 模型可视化\n",
    "model.summary((-1,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、定义训练流程\n",
    "* 数据载入\n",
    "* epoch循环体\n",
    "* 保存评价指标，方便可视化训练\n",
    "* 边训练边验证\n",
    "* 保存中间模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 定义训练流程\n",
    "def train( # 根据fit定制\n",
    "    model, \n",
    "    train_dataset, \n",
    "    test_dataset, \n",
    "    optimizer,\n",
    "    loss,\n",
    "    metric,\n",
    "    epochs=1, \n",
    "    batch_size=1,\n",
    "    save_dir=None, # 是否保存模型\n",
    "    save_freq=1, # 保存频率，单位epoch\n",
    "    verbose=0, # 是否逐行打印输出\n",
    "    log_freq=200, # 打印日志的频率\n",
    "    suffle=True \n",
    "    ):\n",
    "\n",
    "    # 是否构建dataloader取决于传入的是否是Dataloader还是Dataset\n",
    "    if(isinstance(train_dataset,paddle.io.DataLoader)==False):\n",
    "        train_loader = paddle.io.DataLoader(train_dataset, batch_size=batch_size, shuffle=suffle)\n",
    "        test_loader = paddle.io.DataLoader(test_dataset, batch_size=batch_size, shuffle=suffle)\n",
    "\n",
    "    if verbose == 1:\n",
    "        log_freq = 1\n",
    "    print('start training ... ')\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "\n",
    "    train_iter = 0\n",
    "    test_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "\n",
    "            logits = model(x_data)\n",
    "\n",
    "            # 计算评价指标\n",
    "            correct = metric.compute(logits, y_data)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate() # 这里是累积的，返回的是平均的acc，格式list\n",
    "            \n",
    "            l = loss(logits, y_data)\n",
    "            avg_loss = paddle.mean(l)\n",
    "\n",
    "            log_writer.add_scalar(tag = 'train/loss', step = train_iter, value = avg_loss.numpy()[0])\n",
    "            # log_writer.add_scalar(tag = 'top1 acc', step = iter, value = acc[0])\n",
    "            # log_writer.add_scalar(tag = 'top5 acc', step = iter, value = acc[1])\n",
    "\n",
    "            if batch_id % log_freq == 0:\n",
    "                print(\"[train] epoch: {}, batch_id: {}, loss is: {:.4f}, top1 acc: {:.4f}, top5 acc: {:.4f}\".format(epoch, batch_id, avg_loss.numpy()[0], acc[0], acc[1]))\n",
    "                # log_writer.add_scalar(tag = 'loss', step = iter, value = avg_loss.numpy())\n",
    "                log_writer.add_scalar(tag = 'train/top1_acc', step = train_iter, value = acc[0])\n",
    "                log_writer.add_scalar(tag = 'train/top5_acc', step = train_iter, value = acc[1])\n",
    "                metric.reset() # 训练时每输出一次更新一次acc\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "            train_iter += 1\n",
    "        \n",
    "        metric.reset() # 避免有累积\n",
    "        # 每轮后验证一下模型效果\n",
    "        model.eval() # 修改为评估模式\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = data[1]\n",
    "\n",
    "            logits = model(x_data)\n",
    "\n",
    "            # 计算评价指标\n",
    "            correct = metric.compute(logits, y_data)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate() # 这里是累积的，后面需要平均一下\n",
    "            \n",
    "            l = loss(logits, y_data)\n",
    "            avg_loss = paddle.mean(l)\n",
    "\n",
    "            losses.append(l.numpy())\n",
    "            log_writer.add_scalar(tag = 'eval/loss', step = test_iter, value = avg_loss.numpy())\n",
    "            # log_writer.add_scalar(tag = 'top1 acc', step = iter, value = acc[0])\n",
    "            # log_writer.add_scalar(tag = 'top5 acc', step = iter, value = acc[1])\n",
    "            test_iter += 1\n",
    "\n",
    "        log_writer.add_scalar(tag = 'eval/top1_acc', step = epoch, value = acc[0])\n",
    "        log_writer.add_scalar(tag = 'eval/top5_acc', step = epoch, value = acc[1])\n",
    "            \n",
    "        avg_loss = np.mean(losses)\n",
    "        # print(avg_loss)\n",
    "        print(\"[test] epoch: {}, loss is: {:.4f}, top1 acc: {:.4f}, top5 acc: {:.4f}\".format(epoch, avg_loss, acc[0], acc[1]))\n",
    "        metric.reset() # 避免有累积\n",
    "\n",
    "        # 保存模型\n",
    "        if save_dir is not None:\n",
    "            if epoch+1 == epochs:\n",
    "                paddle.save(model.state_dict(), '{}/{}.pdparams'.format(SAVE_DIR, 'final'))\n",
    "                paddle.save(optimizer.state_dict(), '{}/{}.pdopt'.format(SAVE_DIR, 'final'))\n",
    "                print(\"epoch {}: Model has been saved in {}.\".format('final', SAVE_DIR))\n",
    "            if epoch % save_freq == 0:\n",
    "                paddle.save(model.state_dict(), '{}/{}.pdparams'.format(SAVE_DIR, epoch+1))\n",
    "                paddle.save(optimizer.state_dict(), '{}/{}.pdopt'.format(SAVE_DIR, epoch+1))\n",
    "                print(\"epoch {}: Model has been saved in {}.\".format(epoch+1, SAVE_DIR))\n",
    "        model.train() # 转回train模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、编写主函数\n",
    "* 定义优化器、损失函数和评价指标\n",
    "* 初始化参数，并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数入口\n",
    "# if __name__ == '__main__': # 程序入口\n",
    "\n",
    "# 用于模型保存和可视化参数的路径（交互条件下__file__失效）。写入绝对路径\n",
    "# filepath, filename = os.path.split(os.path.realpath(__file__))\n",
    "# stem, suffix = os.path.splitext(filename) # filename .py\n",
    "# filepath = '/home/aistudio/work' # 写自己的绝对路径或者使用相对路径\n",
    "filepath = './work' # 使用相对路径\n",
    "stem = 'cifar10'\n",
    "SAVE_DIR = '{}/model/{}'.format(filepath, stem)\n",
    "visualdl = paddle.callbacks.VisualDL(log_dir='{}/visualdl_log/{}'.format(filepath, stem))\n",
    "log_writer = LogWriter(logdir='{}/visualdl_log/{}'.format(filepath, stem))\n",
    "# print(SAVE_DIR)\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = MyDateset(mode='train')\n",
    "test_dataset = MyDateset(mode='eval')\n",
    "\n",
    "# 模型初始化\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# 定义优化器、损失函数和评价指标\n",
    "scheduler = paddle.optimizer.lr.ReduceOnPlateau(learning_rate=0.01, factor=0.5, patience=5, verbose=True)\n",
    "t_optimizer = paddle.optimizer.Momentum(learning_rate=scheduler, momentum=0.9, parameters=model.parameters(), weight_decay=0.001) # Momentum收敛快\n",
    "t_loss = paddle.nn.CrossEntropyLoss()\n",
    "t_metric = paddle.metric.Accuracy(topk=(1, 5))\n",
    "\n",
    "# 自己增加epoch来观察Acc能到多少，可以改变batch_size，优化器optimizer，损失函数loss以及评价方式metric\n",
    "# 这个训练器可以自己封装一下，以后直接调用，需要自定义也只需要改很少的部分\n",
    "train(model, train_dataset, test_dataset, epochs=3, batch_size=32, optimizer=t_optimizer, loss=t_loss, metric=t_metric, save_dir=SAVE_DIR, save_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、可视化\n",
    "由于BML自带可视化工具，直接可以用log来可视化\n",
    "自己在本地用pip安装visualdl工具\n",
    "然后新开一个终端，用下面命令开启可视化服务\n",
    "```bash\n",
    "visualdl --logdir ./work/visualdl_log/cifar10 --port 8080\n",
    "```\n",
    "在打开浏览器，访问http://localhost:8080"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bcc3c2c8822f69a085da2af0f573da5b80461a402674f5f185203e6fa2a51e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('autoans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
